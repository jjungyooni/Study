# 자연어와 단어의 분산 표현

## 자연어 처리란?
* 우리가 평소에 쓰는 말 -> 자연어 
* Natural Language Processing 자연어처리 -> NLP
### 단어의 의미
 우리의 말은 '문자'로 구성되며, 말의 의미는 '단어'로 구성됩니다.  즉, 자연어를 컴퓨터에게 이해시키는 데는 무엇보다 '단어의 의미'를 이해시키는게 중요 

* 시소러스 활용 기법(유의어 사전)
* 통계 기반 기법
* 추론 기반 기법(word2vec) 

## 시소러스
시소러스란 유의어 사전으로, '뜻이 같은 단어', '뜻이 비슷한 단어' 가 한 그룹으로 분류되어 있다.

### WordNet
자연어 처리 분야에서 가장 유명한 시소러스 

### 시소러스의 문제점
시소러스는 사람이 수작업으로 레입르링 하는 방식으로, 아래와 같은 결점 존재

* 시대 변화에 대응이 힘듦
* 인건비
* 미묘한 차이를 표현하기 힘듦 

> 자연어 처리뿐 아니라, 다양한 분야에서 딥러닝이 실용화 되면서 사람의 개입을 최소로 줄이고 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 바뀌고 있다. 

## 통계 기반 기법
대량의 텍스트 데이터(말뭉치) 를 이용해 자동으로, 효율적으로 그 핵심을 추출하는 것

### 파이썬으로 말뭉치 전처리하기
유명한 말뭉치들은 -> 위키백과, 구글뉴스 혹은 셰익스피어 나 나쓰메 소세키 같은 대문호의 작품들

이번 말뭉치로 활용할 예시 문장은 아래와 같다. 실전에선 이 text 에 수천, 수만 개가 넘는 문장이 담겨있다.
```python
>>> text = 'You say goodbye and I say hello.'
```

#### 텍스트를 단어 단위로 분할
```python
>>> text = text.lower()   # 소문자 변환 
>>> text = text.replace('.',' .') # 마침표 분리 
>>> text
'you say goodbye and i say hello .'
>>> words = text.split(' ') # 공백 기준으로 분할
>>> words
['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', ',']

# 정규표현식을 이용하면 더 현명하고 범용적인 방법이다.
# re.split('(\W+)?',text)
```

이대로 이용하기엔 여러 면에서 불편하다, 그래서 단어에 ID 를 부여하고, ID 의 리스트로 이용할 수 있도록 한 번 더 손질.

```python
>>> word_to_id = {}
>>> id_to_word = {}
>>>
>>> for word in words:
...   if word not in word_to_id:
...       new_id = len(word_to_id)
...       word_to_id[word] = new_id
...       id_to_word[new_id] = word
>>> id_to_word
{0: 'you', 1: 'say', ...}
>>> word_to_id
{'you': 0, 'say': 1, ...}

>>> id_to_word[1] 
'say'
>>> word_to_id['hello']
5

# 내포 표기를 사용하여 배열로 변환 -> 반복문 처리를 간단하게 쓰기위한 기법 
>>> corpus = [word_to_id[w] for w in words]
```

### 단어의 분산 표현
'색'을 벡터로 표현하듯 '단어의 의미'를 정확하게 파악할 수 있는 벡터표현

* 분포 가설 -> '단어의 의미는 주변 단어에 의해 형성된다' 라는 아이디어에서 시작

* 동시발생 행렬 -> 
주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법 -> 통계기반 기법
```
you 라는 단어를 생각하면 [0,1,0,0,0,0,0] 라는 벡터로 표현 가능
say 라는 단어는 [1,0,1,0,1,1,0]
```
### 벡터 간 유사도 
* 단어 벡터의 유사도를 나타낼 때는 코사인 유사도를 자주 이용 -> 두 벡터가 가리키는 방향이 얼마나 비슷한가, **벡터를 정규화하고 내적을 구하는 것**

* 책에 주어진 식을 이용하면 'you'와 'i' 의 코사인 유사도는 0.7 정도로 유사성이 크다 고 말할 수 있다. 

* 유사단어의 랭킹을 표시하면 'you' 와 유사한 상위로 'i','goodbye','hello' 가 나온다. 이는 데이터가 작아서 이렇게 나온것이다. 


## 통계 기반 기법 개선하기
### 상호정보량 
사실 발생 횟수로만 판단하는 것은 그리 좋은 방법은 아니다. 예를 들어 'the'와 같은 관사를 들 수 있다. ...the car... 라고 자주 등장하지만, the 와 car는 강한 관련성을 갖는다고 말 할 수없다. 

-> 이 문제를 해결하기 위해 점별 상호정보량 (PMI) 라는 척도를 사용
-> 두 단어의 동시발생횟수가 0 이면 음의 무한대로 가는 문제가 있어서 

-> 실제로 구현할 때는 양의 상호정보량 (PPMI) 을 사용 
-> PMI 가 음수일때는 0으로 취급하는 것.

-> 위에서 구한 동시발생 행렬을 PPMI 로 변환해서 더 좋은 척도로 이뤄진 행렬을 손에쥐었다!! 하지만 이도 큰 문제가 있었으니

-> 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제와 노이즈에 약하고 견고하지 못하는 약점 (중요도가 높은 몇몇 데이터를 빼고는 원소 대부분 0으로 표현됨 -> 희소벡터) 

-> 그래서 차원감소가 필요!

### 차원 감소
차원감소를 위해 특잇값분해(SVD) 이용


### PTB 데이터셋
너무 크지는 않고 적당한 우리가 앞으로 사용할 말뭉치 -> 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용


# 정리
* 컴퓨터에게 '단어의 의미'를 이해시키기를 주제로 공부
* 그런 방법 중 하나인 시소러스 방법은 수작업으로 힘들고, 인건비가 많이드는 등의 한계가 있다.
* 말뭉치로부터 단어의 읨리르 자동으로 추출하고, 그 읨리ㅡㄹ 벡터로 표현하는 통계 기반 기법도 살펴봄
    * 단어의 동시발생 행렬을 만들고
    * PPMI 행렬로 변환한 다음
    * 안전성을 높이기 위해 SVD를 이용해 차원을 감소 시킨다.
* 벡터 간의 유사도를 측정하는 cos_similiarity() 
* 유사 단어 랭킹을 표시하는 most_similar() 함수 들도 봤다
* 위의 두 함수는 앞으로도 이용되니 참고해볼것